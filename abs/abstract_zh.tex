\begin{abstractzh}

隨著人工智慧技術的快速發展，大型語言模型（LLM）已廣泛應用於複雜問題解決，但現有的雲端推論方式面臨網路依賴、隱私風險和高成本等挑戰。邊緣設備雖具備本地執行能力，但受限於記憶體與運算資源，難以直接部署LLM。小型語言模型（SLM）雖可在邊緣設備運行，但在複雜推理任務上的準確度與反應速度往往不足。

本研究提出一種多SLM協作架構，旨在提升邊緣推理的準確率。該架構包含三個主要創新：首先，設計自適應粒度任務分解機制，根據邊緣設備的硬體配置動態將用戶查詢分解為子任務，提升推論效率；其次，建立協作式聚合機制，採用主從式SLM架構，主節點負責聚合多個工作節點的中間結果，確保結果的穩健性與一致性；最後，實作完整的端到端SLM推論流程，涵蓋任務分解、平行執行與結果聚合，證明複雜LLM任務可在無需雲端資源的情況下於邊緣設備上完成。

實驗結果顯示，本研究所提出的多SLM協作架構能有效提升邊緣推理的準確率，同時保持較低的延遲和資源消耗。該架構為邊緣設備上的智能應用提供了新的解決方案，具有重要的實用價值和應用前景。

\vspace{1cm}
\noindent\textbf{關鍵詞：}邊緣計算、小型語言模型、協作架構、任務分解、推論優化

\end{abstractzh}