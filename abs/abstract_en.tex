\begin{abstract}

With the rapid development of artificial intelligence technology, Large Language Models (LLMs) have been widely applied to complex problem-solving tasks. However, existing cloud-based inference approaches face challenges such as network dependency, privacy risks, and high costs. While edge devices possess local execution capabilities, they are constrained by limited memory and computational resources, making it difficult to directly deploy LLMs. Small Language Models (SLMs) can run on edge devices but often lack sufficient accuracy and response speed for complex reasoning tasks.

This research proposes a collaborative multi-SLM architecture aimed at enhancing accuracy in edge inference. The architecture incorporates three main innovations: First, an adaptive granularity task decomposition mechanism that dynamically breaks down user queries into subtasks based on edge device hardware configurations, improving inference efficiency. Second, a collaborative aggregation mechanism employing a master-slave SLM architecture where the master node aggregates intermediate results from multiple worker nodes, ensuring robustness and consistency of results. Finally, a complete end-to-end SLM inference pipeline covering task decomposition, parallel execution, and result aggregation, demonstrating that complex LLM tasks can be completed on edge devices without requiring cloud resources.

Experimental results show that the proposed collaborative multi-SLM architecture effectively improves edge inference accuracy while maintaining low latency and resource consumption. This architecture provides a new solution for intelligent applications on edge devices, offering significant practical value and application prospects.

\vspace{1cm}
\noindent\textbf{Keywords:} Edge Computing, Small Language Models, Collaborative Architecture, Task Decomposition, Inference Optimization

\end{abstract}