# Paper Writings 

## Introduction
### Motivation
* Emergence of deploying SLMs on edge devices 
    * As AI applications increasingly demand real-time processing and privacy-preserving measures, many companies have started prioritizing the deployment of small language models (SLMs) on edge devices. The attractive combination of low-cost embedded hardware and capable SLM performance has led industry leaders like NVIDIA, Google, and Intel to fully invest in more accessible edge AI solutions.
        > Table 1 
        * The edge AI accelerator market itself is growing rapidly: from around US$7.7B in 2024 to a projected US$38.4B by 2030 at a CAGR of over 30% 
            > ![image](https://hackmd.io/_uploads/SJOG5HFBge.png)
* Collaboration and Reasoning Limitations of SLMs
    * Existing work demonstrates that multi-agent strategies MoA and self-MoA significantly enhance model accuracy: MoA boosts GPT‑4o from 57.5 % to 65.1 % (+10 %), while self-MoA improves WizardLM and LLaMA‑3‑8B by +23.7 % (53.1 ➜ 65.7 %) and +7.3 % (66.4 ➜ 71.3 %), respectively. However, applying SLMs within multi-agent frameworks shows diminishing returns, hinting at underlying bottlenecks.
    * Analysis from Minions reveals that SLMs struggle with long-context and multi-step tasks: the naive Minion protocol reduces cloud cost by ~30× but recovers only ~87 % accuracy; even with the more refined MinionS protocol, achieving ~97.9 % accuracy, cost reduction improves merely by 5.7×. This highlights token size and task complexity as critical factors degrading SLM performance.
        * ![image](https://hackmd.io/_uploads/rkzZbL9Sxx.png)
    * Furthermore, RULER clearly demonstrates that SLMs are highly sensitive to increasing input token sizes. Models such as Llama2-7B, LWM-7B, and LongAlpaca-13B experience rapid performance degradation as context length grows, with accuracy dropping far below baseline beyond 16K or 32K tokens. In contrast, larger models like Gemini-1.5-Pro and GPT-4 maintain robust performance even at much longer input lengths. These results highlight that SLMs face significant challenges in long-context scenarios, underscoring the need for further research and optimization when deploying SLMs on tasks involving extended input sequences.
        * ![image](https://hackmd.io/_uploads/r16ZZLcHge.png)
### Problem Definition
> 這裡要聚焦你要解決的具體問題，明確陳述現有方法的不足，和你專注的挑戰。

Certainly! Here’s a professional, academic English rewrite of your problem definition section:

---

## Problem Definition

Mainstream distributed inference systems predominantly adopt model parallelism or pipeline parallelism, splitting a single large LLM across multiple devices to perform inference. Typical implementations include:
* **llama.cpp RPC Server:** Supports heterogeneous devices and distributed model parameters, but suffers from synchronization bottlenecks and excessive memory allocation.
* **DeepSpeed ZeRO-Inference:** Relies on efficient communication and parameter partitioning, making it suitable for datacenter clusters; however, it does not address inter-device latency or platform heterogeneity bottlenecks \[2].
* **vLLM:** Focused on single-machine and multi-GPU optimization, such as with PagedAttention KV-cache management, but is not designed for cross-device deployment or bandwidth-constrained environments \[3].

**However, this strategy comes with several limitations:**
1. **High Communication and Synchronization Overhead** — Model and pipeline parallelism require frequent exchange of high-dimensional activation tensors between nodes. When commodity networks such as 1 GbE or Wi-Fi are used, synchronization delays can severely degrade throughput (e.g., in llama.cpp’s RPC server, 25.5 ms of network latency restricts generation to only \~5–7 tokens per second) \[1].
2. **Strict Hardware Requirements and Lack of Heterogeneity Support** — These approaches typically assume datacenter-grade environments (GPUs with NVLink or InfiniBand interconnects), making them unsuitable for bandwidth-limited or heterogeneous edge and mobile deployments.

In contrast, collaborative multi-SLM (small language model) architectures offer promising alternatives, but their relative advantages remain underexplored. This study aims to systematically compare such systems against conventional distributed LLM inference along key axes including communication overhead, latency and throughput, hardware requirements, and fault tolerance. Our analysis is grounded in the following perspectives:
1. **Communication Bottlenecks and Latency:** Emphasizing how synchronization delay in low-bandwidth environments severely constrains distributed inference efficiency.
2. **Hardware and Environmental Constraints:** Examining the reliance of different architectures on high-performance interconnects and homogeneous hardware.
3. **Resource Management and Fault Tolerance:** Discussing the impacts of improper memory allocation and node failures on overall system robustness.

Building on these insights, this work proposes a collaborative multi-SLM system and explores its performance advantages and challenges with respect to the above criteria.




### Contribution
> 這裡明確列出你論文的創新點、技術貢獻與學術價值。建議條列式（或用短段落），具體說你「做了什麼」、「和前人有何不同」。


- [ ] Paper research
    - [ ] Problem Definition
            
        

## Resources

| 項目             | Distributed LLM 推論  | 你的 Multi-SLM Multi-Agent 架構 |
| -------------- | ------------------- | --------------------------- |
| **推論模式**       | 分散式部署單一大模型          | 每個節點都是獨立的小模型                |
| **通訊需求**       | 非常高 (模型參數或KV-cache) | 很低（僅需傳遞token-level資訊）       |
| **資料傳輸量**      | 大量 (GB級)            | 微量（KB\~MB級）                 |
| **對傳輸速度依賴性**   | 極高                  | 低                           |
| **適用裝置種類**     | 高效裝置（GPU/強大CPU）     | 低功耗裝置（Raspberry Pi級別）       |
| **架構穩定性**      | 易受通訊瓶頸影響            | 幾乎不受影響（傳輸資料極少）              |
| **Scaling 難度** | 高（受限於高速通訊成本）        | 低（只要加裝置即可）                  |
