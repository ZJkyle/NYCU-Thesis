\chapter{Introduction}
\label{chapter:intro}

\section{Motivation}
Large Language Models (LLMs) have revolutionized natural language processing by delivering strong performance across complex reasoning and understanding tasks. These capabilities have catalyzed the widespread adoption of LLMs in domains ranging from healthcare and finance to software engineering. However, the dominant paradigm for LLM inference remains tightly coupled to powerful, centralized cloud infrastructure.

This cloud-centric approach introduces several practical and strategic challenges:

\textbf{Connectivity Dependency:} Reliable, high-bandwidth internet access is required to interact with cloud-based LLM APIs, rendering this approach unsuitable for mobile, remote, or intermittently connected environments.

\textbf{Privacy Concerns:} Transmitting sensitive or proprietary user data to third-party servers can conflict with privacy requirements or regulatory constraints, especially in domains such as healthcare or personal productivity.

\textbf{Cost and Scalability:} The operational expense of invoking commercial LLM APIs can quickly become prohibitive, particularly for applications requiring long-context reasoning or high query volume, where costs scale linearly with usage.

The recent emergence of Small Language Models (SLMs) offers a promising path toward democratizing language intelligence. By enabling on-device or edge deployment, SLMs address the critical needs of responsiveness, privacy, and autonomy. However, practical edge devices (e.g., Raspberry Pi 5, Jetson Orin Nano) are commonly equipped with constrained physical memory (typically 4–8GB RAM), which limits the feasible model size to around 7 billion parameters—even with optimization.

This hardware bottleneck presents a central research gap:

\textit{How can we harness the benefits of on-device inference—privacy, responsiveness, and cost-effectiveness—while overcoming the inherent resource limitations of edge hardware?}

Furthermore, while smaller models can handle simple tasks, enabling SLMs to support complex, data-intensive reasoning remains an open challenge. Existing solutions often sacrifice accuracy or require extensive task-specific engineering. This motivates the exploration of collaborative, hybrid, or distributed inference strategies that combine the strengths of both local and remote resources—an emerging area exemplified by recent work (Narayan et al., 2025) \cite{Narayan2025Minions}, but with substantial open questions regarding efficiency, quality, and real-world deployability.

\section{Problem Definition}
The core challenge addressed in this thesis is the sub-optimal performance of SLMs when operating independently on resource-constrained edge devices. While SLMs are well-suited for simple tasks, their utility diminishes in real-world applications such as on-site industrial monitoring or interactive healthcare assistants, where complex reasoning and low latency are non-negotiable. In these scenarios, performance bottlenecks become critical pain points, manifesting in two major, quantifiable aspects:

\begin{itemize}
    \item \textbf{Accuracy Degradation:} For complex reasoning tasks, the performance of SLMs is significantly lower than that of their cloud-based counterparts. For instance, in financial report analysis, an SLM might only achieve 60-70\% of the accuracy of state-of-the-art LLMs (DeepSeek-AI, 2024) \cite{DeepSeek-RL}, rendering it unreliable for mission-critical decisions.
    \item \textbf{Latency Bottlenecks:} A common strategy to enhance edge capability is to distribute workloads across multiple devices. However, this introduces severe communication overhead. In preliminary tests using an RPC-based framework, we observed that the inference throughput dropped to merely 1/13th of its single-device performance (Gerganov, 2023) \cite{LlamaCpp-Issue}, making it unsuitable for real-time applications.
\end{itemize}

Existing approaches, whether employing distributed SLMs or hybrid local-cloud models, partially address these issues but often fail to strike an effective balance between accuracy, latency, and resource utilization. They typically force a trade-off, sacrificing real-time responsiveness for higher accuracy, or vice-versa, especially in environments with heterogeneous device capabilities and unstable networks.

This thesis proposes a novel collaborative framework designed specifically to resolve this accuracy-latency trade-off. By dynamically decomposing complex tasks and intelligently allocating them between on-device SLMs and powerful cloud LLMs, our method emphasizes adaptive, fine-grained collaboration between heterogeneous agents. This approach directly tackles the multi-objective optimization challenge that existing work has yet to solve, aiming to deliver both high-fidelity reasoning and real-time performance in practical edge deployments.

\section{Contributions}
To overcome the limitations of on-device SLMs, this thesis introduces a collaborative multi-model framework that intelligently orchestrates tasks between edge devices and the cloud. Our primary contributions are threefold:

\begin{enumerate}
    \item \textbf{Adaptive Granularity Task Decomposition:} We introduce an adaptive granularity scheduler that intelligently and dynamically decomposes user queries into sub-tasks and context chunks. This scheduler takes into account the heterogeneous hardware configurations and real-time resource constraints of the participating edge devices, optimizing the workload distribution for parallel execution and maximizing throughput.

    \item \textbf{Collaborative Aggregation for Robust Output:} We design a master-worker aggregation mechanism where a master SLM collates and synthesizes intermediate results generated by worker SLMs. This collaborative process produces a final, coherent output, ensuring robustness and consistency even in the presence of partial or incomplete responses from worker nodes, thereby enhancing the overall reliability of the system.

    \item \textbf{End-to-End Edge SLM Inference Pipeline:} We demonstrate a practical, fully on-device SLM inference pipeline that seamlessly integrates task decomposition, parallel execution, and result aggregation. This end-to-end implementation validates the feasibility of processing complex, LLM-scale tasks on a network of resource-constrained devices without any reliance on cloud resources, paving the way for more powerful and private edge AI applications.
\end{enumerate}

\section{Thesis Organization}
The remainder of this thesis is structured as follows. Chapter 2 provides a review of related work in edge computing and language model inference. Chapter 3 details the design of our proposed collaborative architecture. Chapter 4 describes the implementation of the end-to-end pipeline. Chapter 5 presents the experimental evaluation and results. Finally, Chapter 6 concludes the thesis and discusses future research directions.