\chapter{Background and Related Work}
\label{chapter:background}

\section{Background}
\subsection{The Rise and Challenges of Large Language Models}
Large Language Models (LLMs), such as GPT and Gemini \cite{Vaswani2017Attention}, have set new standards in natural language understanding, reasoning, and generation. Their remarkable performance has spurred applications in fields ranging from code generation to knowledge-intensive question answering. However, this progress comes at the cost of massive computational and memory requirements, making LLM inference expensive and inaccessible for latency- or privacy-sensitive applications. To address these limitations, the research community has pursued two main directions: (1) parameter-efficient training and model compression techniques such as knowledge distillation, pruning, and quantization, and (2) inference-stage system optimization including efficient attention mechanisms, KV cache optimization, and distributed deployment strategies.

\subsection{Small Language Models (SLMs) and Edge Deployment}
The development of SLMs—such as Llama-3.2-3B \cite{Touvron2023LLaMA} and DeepSeek-R1-Distill \cite{DeepSeek-RL}—offers an alternative path by enabling on-device inference on resource-constrained hardware such as NVIDIA Jetson series and other embedded systems. Although SLMs can be deployed locally and address privacy and connectivity issues, their performance often lags behind LLMs, particularly in complex reasoning or long-context tasks. This challenge motivates the need for innovative techniques that can maximize the utility of SLMs at the edge while minimizing dependence on cloud-based LLMs. Collaborative strategies between local and remote models—including dynamic task decomposition, intelligent workload distribution, and hybrid inference architectures—are increasingly viewed as promising solutions \cite{Narayan2025Minions}.

\section{Related Work}
\subsection{Model Compression and Inference Optimization}
A major line of research has focused on reducing the size and computational requirements of LLMs while retaining most of their capabilities. Techniques such as knowledge distillation \cite{DeepSeek-RL}, pruning, and quantization have produced a new generation of SLMs that are deployable on edge devices. The Open R1 project \cite{HuggingFace2024OpenR1} further demonstrates the effectiveness of these compression approaches, providing high-performing models with drastically reduced resource footprints.

Complementing model-centric techniques, significant efforts have been directed at system-level and inference optimization to enhance the throughput and efficiency of LLM serving. A prominent line of work focuses on optimizing the Key-Value (KV) cache. For instance, vLLM introduces PagedAttention \cite{Kwon2023PagedAttention}, which manages the KV cache like virtual memory to mitigate fragmentation and enable efficient sharing. Building on this, methods like H2O (Heavy Hitter Oracle) \cite{Zhang2023H2O} and Scissorhands \cite{Liu2023Scissorhands} selectively evict less important tokens from the KV cache, achieving significant memory reduction with minimal impact on quality. More recently, PyramidInfer \cite{Zhang2024PyramidInfer} proposed a pyramid-structured KV cache that retains fine-grained information for recent tokens while summarizing older ones, further improving throughput. These system-level optimizations are crucial as they are largely orthogonal to and compatible with model compression techniques.

\subsection{Distributed Inference and Task Decomposition}
Another significant research direction explores distributing LLM inference across multiple devices to overcome the limitations of a single machine. Task decomposition is a core strategy in this paradigm, enabling complex problems to be broken down into smaller, manageable sub-tasks that can be processed in parallel or by specialized agents. For instance, Amazon Science outlines high-level strategies for splitting complex queries into targeted prompts suitable for smaller, specialized models.

This approach is central to hierarchical architectures, such as the one proposed by Luan et al. (2024) \cite{Luan2024Hierarchical}, which spans cloud, edge, and end devices. Their work demonstrates how multiple LLMs can collaborate to decompose high-level tasks into executable sub-tasks for heterogeneous robot clusters. While effective for strategic planning, many rule-based and similarity-based decomposition methods like these still lack robust empirical evaluation for end-to-end, multi-turn reasoning tasks.

To address the need for more dynamic and cost-efficient collaboration, Mixture-of-Agents (MoA) frameworks have been proposed. The work by Narayan et al. (2025) \cite{Narayan2025Minions} shows how a partnership between on-device SLMs and powerful cloud LLMs can improve overall performance. Their framework uses an adaptive task decomposition strategy: simpler sub-tasks are handled locally by SLMs, while more complex reasoning is offloaded to the cloud. This intelligent balancing of local and remote computation has shown substantial gains in task success rates and latency reduction, highlighting a broader shift from single-model execution to collaborative, multi-agent systems.

\subsection{Mixture-of-Agents and Aggregation Paradigms}
Recent research has increasingly focused on the Mixture-of-Agents (MoA) paradigm, which seeks to enhance the robustness and capability of language model inference by leveraging the diversity and complementary strengths of multiple agents. This direction is especially relevant for distributed and collaborative inference scenarios, such as edge-cloud systems or heterogeneous device clusters.

Wang et al. (2024) \cite{Wang2024MoA} demonstrate that aggregating outputs from diverse language models—using mechanisms like voting or scoring—can significantly improve performance on reasoning and instruction-following tasks. Building on this, Zhang et al. (2024) \cite{Zhang2024SMoA} introduce the Sparse Mixture-of-Agents (SMoA) framework, which incorporates early stopping and dynamic role assignment to increase inference throughput and efficiency.

However, a critical perspective from Li et al. (2025) \cite{Li2025RethinkingMoA} reveals that intra-model diversity (i.e., generating multiple outputs from a single, high-quality model, termed Self-MoA) often outperforms naive mixtures of heterogeneous agents. Their findings highlight that including low-quality agents can degrade overall performance, underscoring the need for careful agent selection and aggregation strategies.

Despite these advances, current MoA frameworks often overlook device heterogeneity, resource constraints, and the need for adaptive agent assignment in real-world edge scenarios. These open challenges motivate the selective aggregation and dynamic collaboration mechanisms developed in this thesis, which aim to create robust, scalable, and resource-aware inference pipelines.

\section{Summary}
In summary, the pursuit of efficient and powerful language model inference has advanced along several key fronts. The development of Small Language Models (SLMs), enabled by model compression and inference-stage optimizations like advanced KV cache management, has made high-quality language processing viable on edge devices. Concurrently, distributed inference and task decomposition strategies have shown promise in overcoming single-device limitations, while Mixture-of-Agents (MoA) paradigms have further explored how to aggregate diverse outputs to enhance reasoning and robustness.

However, a critical gap exists at the intersection of these fields. While each area has made significant contributions, their integration in resource-constrained, real-world edge environments remains a substantial challenge. Current task decomposition methods often lack robust evaluation for complex reasoning, and MoA frameworks frequently overlook the practical constraints of device heterogeneity and the risk of performance degradation from naive agent aggregation.

This thesis builds upon these advances by proposing an adaptive, collaborative SLM inference pipeline. It aims to bridge the gap between local efficiency and LLM-scale performance by introducing selective aggregation and dynamic, resource-aware collaboration mechanisms tailored for practical edge computing scenarios. 