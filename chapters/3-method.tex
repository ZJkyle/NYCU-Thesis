\chapter{Proposed Method}
\label{chapter:method}

This chapter details the design and implementation of the proposed collaborative multi-SLM framework. The architecture is engineered to overcome the accuracy limitations of standalone SLMs on edge devices by employing adaptive task decomposition and a robust collaborative aggregation mechanism to enhance overall inference performance.

\section{System Architecture}

The proposed framework employs a master-worker architecture, as illustrated in Figure~\ref{fig:system_architecture}. This design comprises a single master node that orchestrates multiple worker nodes, leveraging intelligent task decomposition and collaborative result aggregation to achieve efficient and accurate edge-based inference.

\begin{figure}[h]
  \centering
  \includegraphics[height=!,width=0.8\linewidth,keepaspectratio=true]%
  {figures/System_arch.png}
  \caption[System Architecture of the Collaborative Multi-SLM Framework]{The system architecture of the collaborative multi-SLM framework, illustrating the relationship between the master and worker nodes and the overall workflow of task decomposition and aggregation.}
  \label{fig:system_architecture}
\end{figure}

The primary components of the system architecture are:

\begin{itemize}
    \item \textbf{Master Node:} Responsible for receiving user queries, performing task decomposition, coordinating worker node execution, and aggregating the final results.
    \item \textbf{Worker Nodes:} Execute assigned sub-tasks using various SLM models and return intermediate results to the master node.
    \item \textbf{Task Decomposer:} A module on the master node that partitions a complex user query into optimized sub-tasks based on hardware configurations and task complexity.
    \item \textbf{Result Aggregator:} A module on the master node that synthesizes intermediate results from multiple worker nodes to produce the final, coherent output.
\end{itemize}

\section{Adaptive Granularity Task Decomposition}

A core innovation of this framework is its adaptive granularity task decomposition mechanism. This mechanism dynamically adjusts the granularity of task decomposition based on the real-time hardware configurations of the edge devices (e.g., available memory, CPU core count) to achieve optimal inference efficiency.

The task decomposition process consists of the following steps:

\begin{enumerate}
    \item \textbf{Hardware Resource Assessment:} The master node first evaluates the available resources of each worker node, including memory capacity and current CPU utilization.
    \item \textbf{Task Complexity Analysis:} The incoming user query is analyzed to estimate its complexity, considering factors such as text length and required reasoning depth.
    \item \textbf{Granularity Decision:} Based on the assessed resources and task complexity, the decomposer determines the most suitable decomposition granularity (e.g., number and size of sub-tasks).
    \item \textbf{Sub-task Generation:} The original query is partitioned into multiple sub-tasks that can be executed in parallel by the worker nodes.
\end{enumerate}

\section{Collaborative Aggregation Mechanism}

The collaborative aggregation mechanism ensures the effectiveness and robustness of the multi-SLM collaboration. Operating within the master-worker paradigm, this mechanism assigns the master node the responsibility of coordinating and aggregating results, while worker nodes focus on executing their designated inference tasks.

Key features of the aggregation process include:

\begin{itemize}
    \item \textbf{Fault Tolerance:} The system can produce a viable output even if some worker nodes fail to return results, ensuring robustness.
    \item \textbf{Consistency:} A consistency-checking protocol is implemented to verify the coherence and reliability of the aggregated result.
    \item \textbf{Scalability:} The architecture supports the dynamic addition or removal of worker nodes, allowing it to adapt to heterogeneous hardware environments.
\end{itemize}

\section{End-to-End Inference Pipeline}

The complete end-to-end inference process is divided into the following distinct phases:

\subsection{Initialization Phase}
Upon system startup, the master node scans the network for available worker nodes. It assesses their hardware specifications and loaded SLM capabilities to build a node registry for efficient task allocation.

\subsection{Query Processing Phase}
When a user query is received, the master node first performs task analysis. It then employs the adaptive granularity strategy to decompose the query into a set of optimized sub-tasks.

\subsection{Parallel Execution Phase}
The generated sub-tasks are distributed among the registered worker nodes. Each node executes its assigned SLM inference task in parallel and transmits the intermediate results back to the master.

\subsection{Result Aggregation Phase}
The master node collects all intermediate results from the worker nodes. These results are then synthesized via the collaborative aggregation mechanism to produce a final, unified output that is delivered to the user.

\section{Implementation Details}

The proposed framework is implemented on top of the \textit{Llama.cpp} library, capitalizing on its high-efficiency inference engine and flexible model-loading capabilities. Key implementation characteristics include:

\begin{itemize}
    \item \textbf{Lightweight Communication:} An efficient Remote Procedure Call (RPC) protocol is used for inter-node communication to minimize network overhead.
    \item \textbf{Dynamic Load Balancing:} Task allocation is dynamically adjusted based on the real-time load of each worker node to prevent bottlenecks.
    \item \textbf{Resource Monitoring:} The framework continuously monitors the resource utilization of each node to ensure system stability and predictable performance.
\end{itemize} 