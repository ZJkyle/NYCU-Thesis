% 主流大型語言模型長文本能力比較表（指定順序，models/length/4k/avg間加雙線）
\rowcolors{2}{gray!15}{white}
\begin{tabular}{l|cc|cccccc|c}
\toprule[1.1pt]
Models & Claimed & Effective & 4K & 8K & 16K & 32K & 64K & 128K & Avg. \\
 & Length & Length & & & & & & & \\
\midrule[1.1pt]
Gemini-1.5-Pro & 1M & $>$128K & \underline{96.7} & \underline{95.8} & \underline{96.0} & \underline{95.9} & \underline{95.9} & \underline{94.4} & \underline{95.8} \\
GPT-4 & 128K & 64K & \underline{96.6} & \underline{96.3} & \underline{95.2} & \underline{93.2} & \underline{87.0} & 81.2 & \underline{91.6} \\
Mixtral (8x22B) & 64K & 32K & \underline{96.3} & \underline{93.4} & \underline{90.9} & 84.7 & 0.0 & 0.0 & 88.8 \\
Qwen2 (72B) & 128K & 32K & \underline{96.9} & \underline{96.1} & \underline{94.1} & 79.7 & 0.0 & 0.0 & 85.9 \\
Llama3.1 (70B) & 128K & 64K & \underline{96.5} & \underline{95.8} & \underline{95.4} & \underline{94.8} & \underline{88.4} & 66.6 & 89.6 \\
Llama3 (70B) & 1M & 16K & \underline{95.4} & \underline{94.4} & \underline{90.9} & 85.4 & 0.0 & 0.0 & 89.0 \\
Phi3-medium (14B) & 128K & 32K & \underline{93.3} & \underline{93.2} & \underline{91.1} & 86.8 & 78.0 & 0.0 & 88.5 \\
Llama3.1 (8B) & 128K & 32K & \underline{95.5} & \underline{93.8} & \underline{91.6} & 87.4 & 84.7 & 77.0 & 88.3 \\
Mistral-v0.2 (7B) & 128K & 16K & \underline{93.6} & \underline{91.2} & 87.2 & 75.4 & 49.0 & 0.0 & 68.4 \\
Llama2 (7B) & 4K & - & 85.6 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\bottomrule[1.1pt]
\end{tabular} 